---
title: "Analysis Exploration: Blood Glucose Dynamics"
author: "Research Team"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    number_sections: true
---

# Introduction

This document explores the nonlinear statistical modeling of short-horizon blood glucose prediction using routinely available features. We compare Linear Regression, Generalized Additive Models (GAM), and Nonlinear Regression (NLS).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Load necessary packages
library(tidyverse)
library(lubridate)
library(skimr)
library(GGally)
library(mgcv)
library(zoo)
# library(minpack.lm) # Uncomment if robust non-linear least squares (nlsLM) is needed
```

# Data Loading and Inspection

```{r load-data}
# Load the dataset using a relative path for reproducibility
# Assuming the file is located in the '../data' directory relative to this Rmd
data <- read.csv("../data/bg_data.csv")

# Quick look at the data structure
head(data)
str(data)
```

## Summary Statistics

```{r summary-stats}
# Data summary
summary(data)

# Detailed skim of the data
skim(data)
```

# Exploratory Data Analysis (EDA)

## Missing Value Analysis

Understanding the extent of missing data is crucial for selecting appropriate imputation strategies.

```{r missing-values}
# Count missing values per column
colSums(is.na(data))

# Percentage of missing values
colMeans(is.na(data)) * 100
```

## Distribution of Variables

We visualize the distribution of all variables to identify skewness and outliers.

```{r distributions, fig.height=8, fig.width=10}
data %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(title = "Distribution of Variables")
```

## Correlation Analysis

```{r correlation, fig.height=10, fig.width=10}
# Pairwise plots to visualize correlations
# Note: This can be computationally intensive for large datasets
ggpairs(data, lower = list(continuous = wrap("points", alpha = 0.3, size = 0.5))) +
  theme_minimal() +
  labs(title = "Pairwise Correlations")
```

## Time Series Awareness

Visualizing the target variable (`bg.1.00`, representing blood glucose 1 hour ahead) over time.

```{r ts-plot}
data %>%
  mutate(index = row_number()) %>%
  ggplot(aes(index, bg.1.00)) +
  geom_line(color = "darkblue", alpha = 0.8) +
  theme_minimal() +
  labs(
    title = "Blood Glucose (1 Hour Ahead) Over Time",
    x = "Time Index",
    y = "Blood Glucose (mg/dL)"
  )
```

## Nonlinearity Check

We investigate the relationship between current mean blood glucose (`bg_mean`) and the target variable to justify the use of GAM and Nonlinear models.

```{r nonlinearity-check}
data %>%
  ggplot(aes(bg_mean, bg.1.00)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  theme_minimal() +
  labs(
    title = "Nonlinear Relationship: Current Mean BG vs Future BG",
    x = "Current Mean BG",
    y = "Future BG (+1:00)"
  )
```

# Data Preprocessing

## Handling Missing Data (Heart Rate)

We observed missing values in `hr_mean`. Given the skewed nature of the data, we use median imputation and add a missingness indicator to preserve information.

```{r imputation}
# Calculate missing percentage for Heart Rate
missing_pct <- mean(is.na(data$hr_mean)) * 100
cat("Missing Percentage for hr_mean:", round(missing_pct, 2), "%\n")

# Create missing indicator and impute with median
hr_median <- median(data$hr_mean, na.rm = TRUE)

data <- data %>%
  mutate(
    hr_missing = ifelse(is.na(hr_mean), 1, 0),
    hr_mean = ifelse(is.na(hr_mean), hr_median, hr_mean)
  )

# Verify no duplicate missing values remain
colSums(is.na(data))
```

Visualizing the effect of imputation:

```{r imputation-viz}
ggplot(data, aes(hr_mean, bg.1.00)) +
  geom_point(alpha = 0.3) +
  facet_wrap(~hr_missing, labeller = labeller(hr_missing = c("0" = "Observed HR", "1" = "Imputed HR"))) +
  theme_minimal() +
  labs(
    title = "Relationship Between Heart Rate and Future Glucose (Observed vs Imputed)",
    x = "Heart Rate (hr_mean)",
    y = "Future BG"
  )
```

## Log Transformations

To handle right-skewed variables (steps, carbs, calories), we apply a `log1p` transformation (log(x + 1)). This stabilizes variance and improves model performance.

```{r log-transformation}
data <- data %>%
  mutate(
    steps_log = log1p(steps_sum),
    carbs_log = log1p(carbs_sum),
    cals_log  = log1p(cals_sum)
  )
```

Visualizing distributions after transformation:

```{r trans-viz, fig.height=6, fig.width=10}
data %>%
  select(steps_sum, steps_log, carbs_sum, carbs_log, cals_sum, cals_log) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(title = "Distributions Before and After Log Transformation")
```

Checking linearity of transformed variables:

```{r log-linearity}
p1 <- ggplot(data, aes(carbs_log, bg.1.00)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess", se = FALSE) +
  theme_minimal() +
  labs(title = "Log(Carbs) vs Future BG", x = "log(1 + carbs)", y = "bg+1:00")

p2 <- ggplot(data, aes(cals_log, bg.1.00)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess", se = FALSE) +
  theme_minimal() +
  labs(title = "Log(Calories) vs Future BG", x = "log(1 + cals)", y = "bg+1:00")

p3 <- ggplot(data, aes(steps_log, bg.1.00)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess", se = FALSE) +
  theme_minimal() +
  labs(title = "Log(Steps) vs Future BG", x = "log(1 + steps)", y = "bg+1:00")

gridExtra::grid.arrange(p1, p2, p3, ncol=3)
```

## Zero-Inflation Check

We check the percentage of zeros in key behavioral variables to decide if zero-indicators are necessary.

```{r zero-check}
cat("Zeros in Carbs:", round(mean(data$carbs_sum == 0) * 100, 2), "%\n")
cat("Zeros in Steps:", round(mean(data$steps_sum == 0) * 100, 2), "%\n")
cat("Zeros in Cals: ", round(mean(data$cals_sum == 0) * 100, 2), "%\n")
# Note: Values < 60% are typically fine without specific zero-inflation handling in simple models.
```

# Modeling Strategy

## Time-Aware Train-Test Split

Since the data is time-series, we must split chronologically rather than randomly to prevent data leakage.

```{r split}
n <- nrow(data)
split_idx <- floor(0.7 * n)

train_data <- data[1:split_idx, ]
test_data  <- data[(split_idx + 1):n, ]

cat("Training Samples:", nrow(train_data), "\n")
cat("Testing Samples: ", nrow(test_data), "\n")

# Visualization of Split
ggplot() +
  geom_line(data = train_data, aes(x = 1:nrow(train_data), y = bg.1.00),
            color = "blue", alpha = 0.6) +
  geom_line(data = test_data, aes(x = (nrow(train_data)+1):n, y = bg.1.00),
            color = "red", alpha = 0.6) +
  theme_minimal() +
  labs(
    title = "Time-Aware Trainâ€“Test Split",
    x = "Time Index",
    y = "bg+1:00"
  ) +
  annotate("text", x = nrow(train_data)/2, y = max(data$bg.1.00, na.rm=T), label = "Train", color = "blue") +
  annotate("text", x = nrow(train_data) + nrow(test_data)/2, y = max(data$bg.1.00, na.rm=T), label = "Test", color = "red")
```

## Model 1: Linear Regression (Baseline)

We start with a standard linear regression model using the transformed features.

```{r linear-model}
lm_baseline <- lm(
  bg.1.00 ~ bg_mean + hr_mean + hr_missing +
            insulin_sum + carbs_log + steps_log + cals_log,
  data = train_data
)

summary(lm_baseline)

# Diagnostics
par(mfrow = c(1, 2))
plot(lm_baseline, which = 1) # Residuals vs Fitted
plot(lm_baseline, which = 2) # QQ Plot
par(mfrow = c(1, 1))
```

### Evaluation (LM)

```{r eval-lm}
test_pred_lm <- predict(lm_baseline, newdata = test_data)

rmse_lm <- sqrt(mean((test_data$bg.1.00 - test_pred_lm)^2))
mae_lm  <- mean(abs(test_data$bg.1.00 - test_pred_lm))

cat("Linear Regression - RMSE:", rmse_lm, "\n")
cat("Linear Regression - MAE: ", mae_lm, "\n")

# Predicted vs Observed Plot
ggplot(data.frame(observed = test_data$bg.1.00, predicted = test_pred_lm), 
       aes(observed, predicted)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(title = "Linear Regression: Observed vs Predicted (Test Set)")
```

## Model 2: Generalized Additive Model (GAM)

We use GAMs to capture nonlinear relationships using smoothing splines.

```{r gam-model}
gam_model <- gam(
  bg.1.00 ~
    s(bg_mean, k = 10) +
    s(hr_mean, k = 10) +
    hr_missing +
    s(insulin_sum, k = 10) +
    s(carbs_log, k = 10) +
    s(steps_log, k = 10) +
    s(cals_log, k = 10),
  data = train_data,
  method = "REML"
)

summary(gam_model)

# Visualize Partial Effects
plot(gam_model, pages = 1, shade = TRUE, seWithMean = TRUE)
```

### Evaluation (GAM)

```{r eval-gam}
test_pred_gam <- predict(gam_model, newdata = test_data)

rmse_gam <- sqrt(mean((test_data$bg.1.00 - test_pred_gam)^2))
mae_gam  <- mean(abs(test_data$bg.1.00 - test_pred_gam))

cat("GAM - RMSE:", rmse_gam, "\n")
cat("GAM - MAE: ", mae_gam, "\n")

ggplot(data.frame(observed = test_data$bg.1.00, predicted = test_pred_gam), 
       aes(observed, predicted)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(title = "GAM: Observed vs Predicted (Test Set)")
```

## Model 3: Parametric Nonlinear Regression (NLS)

We attempt to fit a specific functional form based on domain knowledge or empirical observation.

**Scaling**: `bg_mean` is scaled to stabilize the optimization of the exponential term.

```{r nls-prep}
# Scale bg_mean to avoid singular gradients in NLS
bg_mean_mean <- mean(train_data$bg_mean)
bg_mean_sd   <- sd(train_data$bg_mean)

train_data <- train_data %>%
  mutate(bg_mean_sc = (bg_mean - bg_mean_mean) / bg_mean_sd)

test_data <- test_data %>%
  mutate(bg_mean_sc = (bg_mean - bg_mean_mean) / bg_mean_sd)

# Initial parameter guesses based on Linear Model coefficients
start_vals <- list(
  b0 = mean(train_data$bg.1.00, na.rm = TRUE),
  b1 = 1,                                      
  b2 = 0.1,                                    
  b3 = coef(lm_baseline)["carbs_log"],          
  b4 = abs(coef(lm_baseline)["insulin_sum"]),   
  b5 = coef(lm_baseline)["hr_mean"],            
  b6 = coef(lm_baseline)["hr_missing"],         
  b7 = coef(lm_baseline)["steps_log"]            
)
```

```{r nls-fit}
# Formula: bgt+1 = b0 + b1*exp(b2*bg_mean_sc) + ...
# Note: Using 'port' algorithm for stability

nls_model <- tryCatch({
  nls(
    bg.1.00 ~
      b0 +
      b1 * exp(b2 * bg_mean_sc) +
      b3 * carbs_log -
      b4 * insulin_sum +
      b5 * hr_mean +
      b6 * hr_missing -
      b7 * steps_log,
    data = train_data,
    start = start_vals,
    algorithm = "port",
    control = nls.control(maxiter = 300, warnOnly = TRUE)
  )
}, error = function(e) {
  message("NLS failed to converge: ", e$message)
  return(NULL)
})

if(!is.null(nls_model)) summary(nls_model)
```

# Additional Analysis: Trends and Rolling Averages

We compute rolling averages (window = 1000) to visualize long-term trends in predictors and the target variable.

```{r rolling-avg}
vars <- c("bg_mean", "hr_mean", "insulin_sum", "steps_sum", "carbs_sum", "cals_sum")
window_size <- 1000

# Function to plot rolling average
plot_rolling <- function(data, var_name, window = 1000) {
  roll_col <- paste0(var_name, "_roll")
  data[[roll_col]] <- rollmean(data[[var_name]], k = window, fill = NA, align = "center")
  
  ggplot(data, aes(x = index)) +
    geom_line(aes(y = .data[[var_name]]), alpha = 0.2, color = "gray50") +
    geom_line(aes(y = .data[[roll_col]]), linewidth = 1, color = "blue") +
    theme_minimal() +
    labs(
      title = paste(var_name, ": Raw vs Smoothed (k=", window, ")"),
      x = "Time Index",
      y = var_name
    )
}

# Add index if needed
data_plot <- data %>% mutate(index = row_number())

# Example Plots
p_bg <- plot_rolling(data_plot, "bg_mean")
p_hr <- plot_rolling(data_plot, "hr_mean")

# Display
p_bg
p_hr
```

# Conclusion

This exploration highlights the nonlinear nature of blood glucose dynamics. The GAM model suggests significant nonlinear effects for `bg_mean` and potentially `insulin_sum`. The comparison between LM, GAM, and NLS provides a strong foundation for selecting the simplistic yet effective model for the final formulation.
